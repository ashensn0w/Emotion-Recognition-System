{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Initial Setup\n",
    "\n",
    "The following libraries and modules are imported and initialized for text preprocessing and feature extraction:\n",
    "\n",
    "`pandas`: Library for data manipulation and analysis.\n",
    "\n",
    "`string`: Contains common string operations and constants.\n",
    "\n",
    "`nltk`: Natural Language Toolkit, used for tokenization and stop words.\n",
    "\n",
    "`spacy`: Library for advanced natural language processing.\n",
    "\n",
    "`sklearn.feature_extraction.text.TfidfVectorizer`: For converting text to TF-IDF features.\n",
    "\n",
    "NLTK data (`punkt` and `stopwords`) is downloaded for tokenization and filtering out common stop words.\n",
    "\n",
    "The `en_core_web_trf` model from spaCy is loaded for processing text with transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filipino Stopwords\n",
    "\n",
    "The following list contains commonly used Filipino stopwords. These are words that are typically filtered out in text processing as they do not carry significant meaning in the context of text analysis.\n",
    "\n",
    "These stopwords include common words such as articles, prepositions, and conjunctions that are usually removed during text preprocessing to focus on more meaningful words in text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filipino_stopwords = [\n",
    "    \"a\", \"ako\", \"ang\", \"ano\", \"at\", \"ay\", \"ibang\", \"ito\", \"iyon\", \"ka\",\n",
    "    \"kami\", \"kanila\", \"kanya\", \"kayo\", \"laki\", \"mga\", \"na\", \"ng\", \"ni\",\n",
    "    \"nito\", \"nang\", \"sa\", \"sila\", \"tayo\", \"walang\", \"yung\", \"si\", \"bawat\",\n",
    "    \"kung\", \"hindi\", \"para\", \"dahil\", \"doon\", \"baka\", \"kapag\", \"saan\",\n",
    "    \"sino\", \"siya\", \"tama\", \"yan\", \"o\", \"pala\", \"pero\", \"wala\", \"huwag\",\n",
    "    \"muna\", \"na\", \"naman\", \"pag\", \"sana\", \"tulad\", \"upang\", \"bago\", \n",
    "    \"dati\", \"iba\", \"madami\", \"nakita\", \"pagkatapos\", \"pati\", \"sabi\", \"sana\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `load_dataset`\n",
    "\n",
    "The `load_dataset` function is used to load a dataset from a CSV file. It handles various potential errors during the file loading process.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please check the file path.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"File is empty. Please check the file content.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error parsing file. Please check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset and Extracting Sentences\n",
    "\n",
    "The following code snippet demonstrates how to load a dataset from a CSV file and extract sentences from it.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/sample_dataset.csv'\n",
    "data = load_dataset(file_path)\n",
    "\n",
    "sentences = data['sentence'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `format_list_as_string`\n",
    "\n",
    "The `format_list_as_string` function converts a list of tokens into a formatted string representation.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_as_string(token_list):\n",
    "    return str(token_list).replace(\"'\", '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `print_table`\n",
    "\n",
    "The `print_table` function displays a formatted table using the `rich` library, which provides enhanced terminal output for data visualization.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(data, title=\"Table\", num_samples=5):\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    \n",
    "    table = Table(title=title)\n",
    "    \n",
    "    for col in data.columns:\n",
    "        table.add_column(col)\n",
    "\n",
    "    for _, row in data.head(num_samples).iterrows():\n",
    "        formatted_row = [format_list_as_string(row[col]) if isinstance(row[col], list) else row[col] for col in data.columns]\n",
    "        table.add_row(*map(str, formatted_row))\n",
    "    \n",
    "    console = Console()\n",
    "    console.print(table)\n",
    "\n",
    "print_table(data, title=\"Original Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `convert_to_lowercase`\n",
    "\n",
    "The `convert_to_lowercase` function converts all text in the 'sentence' column of a DataFrame to lowercase.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lowercase(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].str.lower()\n",
    "        \n",
    "        data.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(\"Sentence has been converted to lowercase.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "convert_to_lowercase(data)\n",
    "print_table(data, title=\"Data After Lowercase Conversion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `remove_punctuation`\n",
    "\n",
    "The `remove_punctuation` function removes punctuation from all text in the 'sentence' column of a DataFrame.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "        print(\"Punctuation has been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_punctuation(data)\n",
    "print_table(data, title=\"Data After Punctuation Removal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `remove_numbers`\n",
    "\n",
    "The `remove_numbers` function removes numerical digits from all text in the 'sentence' column of a DataFrame.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].str.replace(r'\\d+', '', regex=True)\n",
    "        print(\"Numbers have been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_numbers(data)\n",
    "print_table(data, title=\"Data After Numbers Removal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `tokenize_sentences`\n",
    "\n",
    "The `tokenize_sentences` function tokenizes each sentence in the 'sentence' column of a DataFrame into individual words.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda x: word_tokenize(x))\n",
    "        print(\"Sentences have been tokenized.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "tokenize_sentences(data)\n",
    "print_table(data, title=\"Data After Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `remove_stopwords`\n",
    "\n",
    "The `remove_stopwords` function removes stopwords from each tokenized sentence in the 'sentence' column of a DataFrame.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        english_stopwords = set(stopwords.words('english'))\n",
    "        all_stopwords = english_stopwords.union(set(filipino_stopwords))\n",
    "        \n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: [word for word in tokens if word.lower() not in all_stopwords])\n",
    "        print(\"Stopwords have been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_stopwords(data)\n",
    "print_table(data, title=\"Data After Stopwords Removal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `lemmatize_tokens`\n",
    "\n",
    "The `lemmatize_tokens` function lemmatizes each token in the 'sentence' column of a DataFrame using spaCy's lemmatization.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: [nlp(token)[0].lemma_ for token in tokens])\n",
    "        print(\"Tokens have been lemmatized.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "lemmatize_tokens(data)\n",
    "print_table(data, title=\"Data After Lemmatization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `join_tokens`\n",
    "\n",
    "The `join_tokens` function joins tokens in each list within the 'sentence' column of a DataFrame back into single sentences.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: ' '.join(tokens))\n",
    "        print(\"Tokens have been joined back into sentences.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "join_tokens(data)\n",
    "print_table(data, title=\"Data After Joining Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `vectorize_with_tfidf`\n",
    "\n",
    "The `vectorize_with_tfidf` function performs TF-IDF vectorization on the 'sentence' column of a DataFrame.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_with_tfidf(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "        tfidf_matrix = vectorizer.fit_transform(data['sentence'])\n",
    "\n",
    "        print(\"TF-IDF Vectorization complete.\")\n",
    "        return tfidf_matrix, vectorizer\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "tfidf_matrix, vectorizer = vectorize_with_tfidf(data)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "tfidf_df['emotion'] = data['emotion'].values\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
