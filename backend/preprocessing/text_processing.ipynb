{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filipino_stopwords = [\n",
    "    \"a\", \"ako\", \"ang\", \"ano\", \"at\", \"ay\", \"ibang\", \"ito\", \"iyon\", \"ka\",\n",
    "    \"kami\", \"kanila\", \"kanya\", \"kayo\", \"laki\", \"mga\", \"na\", \"ng\", \"ni\",\n",
    "    \"nito\", \"nang\", \"sa\", \"sila\", \"tayo\", \"walang\", \"yung\", \"si\", \"bawat\",\n",
    "    \"kung\", \"hindi\", \"para\", \"dahil\", \"doon\", \"baka\", \"kapag\", \"saan\",\n",
    "    \"sino\", \"siya\", \"tama\", \"yan\", \"o\", \"pala\", \"pero\", \"wala\", \"huwag\",\n",
    "    \"muna\", \"na\", \"naman\", \"pag\", \"sana\", \"tulad\", \"upang\", \"bago\", \n",
    "    \"dati\", \"iba\", \"madami\", \"nakita\", \"pagkatapos\", \"pati\", \"sabi\", \"sana\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please check the file path.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"File is empty. Please check the file content.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error parsing file. Please check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/sample_dataset.csv'\n",
    "data = load_dataset(file_path)\n",
    "\n",
    "sentences = data['sentence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_as_string(token_list):\n",
    "    return str(token_list).replace(\"'\", '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                           Original Data                           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                              </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ It is possible for him to finish the project on time. │ fear    │\n",
       "│ Maaari kang umalis.                                   │ anger   │\n",
       "│ I intend to go on a vacation next week.               │ fear    │\n",
       "│ He has already completed his homework.                │ sadness │\n",
       "│ Maari ko syang puntahan.                              │ joy     │\n",
       "└───────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                           Original Data                           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ It is possible for him to finish the project on time. │ fear    │\n",
       "│ Maaari kang umalis.                                   │ anger   │\n",
       "│ I intend to go on a vacation next week.               │ fear    │\n",
       "│ He has already completed his homework.                │ sadness │\n",
       "│ Maari ko syang puntahan.                              │ joy     │\n",
       "└───────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_table(data, title=\"Table\", num_samples=5):\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    \n",
    "    table = Table(title=title)\n",
    "    \n",
    "    for col in data.columns:\n",
    "        table.add_column(col)\n",
    "\n",
    "    for _, row in data.head(num_samples).iterrows():\n",
    "        formatted_row = [format_list_as_string(row[col]) if isinstance(row[col], list) else row[col] for col in data.columns]\n",
    "        table.add_row(*map(str, formatted_row))\n",
    "    \n",
    "    console = Console()\n",
    "    console.print(table)\n",
    "\n",
    "print_table(data, title=\"Original Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence has been converted to lowercase and saved to: ../data/lowercased_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                  Data After Lowercase Conversion                  </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                              </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ it is possible for him to finish the project on time. │ fear    │\n",
       "│ maaari kang umalis.                                   │ anger   │\n",
       "│ i intend to go on a vacation next week.               │ fear    │\n",
       "│ he has already completed his homework.                │ sadness │\n",
       "│ maari ko syang puntahan.                              │ joy     │\n",
       "└───────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                  Data After Lowercase Conversion                  \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ it is possible for him to finish the project on time. │ fear    │\n",
       "│ maaari kang umalis.                                   │ anger   │\n",
       "│ i intend to go on a vacation next week.               │ fear    │\n",
       "│ he has already completed his homework.                │ sadness │\n",
       "│ maari ko syang puntahan.                              │ joy     │\n",
       "└───────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_lowercase(data, file_path='../data/lowercased_data.csv'):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].str.lower()\n",
    "        \n",
    "        data.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(\"Sentence has been converted to lowercase and saved to:\", file_path)\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "convert_to_lowercase(data)\n",
    "print_table(data, title=\"Data After Lowercase Conversion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation has been removed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                  Data After Punctuation Removal                  </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                             </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ it is possible for him to finish the project on time │ fear    │\n",
       "│ maaari kang umalis                                   │ anger   │\n",
       "│ i intend to go on a vacation next week               │ fear    │\n",
       "│ he has already completed his homework                │ sadness │\n",
       "│ maari ko syang puntahan                              │ joy     │\n",
       "└──────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                  Data After Punctuation Removal                  \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ it is possible for him to finish the project on time │ fear    │\n",
       "│ maaari kang umalis                                   │ anger   │\n",
       "│ i intend to go on a vacation next week               │ fear    │\n",
       "│ he has already completed his homework                │ sadness │\n",
       "│ maari ko syang puntahan                              │ joy     │\n",
       "└──────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_punctuation(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "        print(\"Punctuation has been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_punctuation(data)\n",
    "print_table(data, title=\"Data After Punctuation Removal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers have been removed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                    Data After Numbers Removal                    </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                             </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ it is possible for him to finish the project on time │ fear    │\n",
       "│ maaari kang umalis                                   │ anger   │\n",
       "│ i intend to go on a vacation next week               │ fear    │\n",
       "│ he has already completed his homework                │ sadness │\n",
       "│ maari ko syang puntahan                              │ joy     │\n",
       "└──────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                    Data After Numbers Removal                    \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ it is possible for him to finish the project on time │ fear    │\n",
       "│ maaari kang umalis                                   │ anger   │\n",
       "│ i intend to go on a vacation next week               │ fear    │\n",
       "│ he has already completed his homework                │ sadness │\n",
       "│ maari ko syang puntahan                              │ joy     │\n",
       "└──────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_numbers(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].str.replace(r'\\d+', '', regex=True)\n",
    "        print(\"Numbers have been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\\\n",
    "\n",
    "remove_numbers(data)\n",
    "print_table(data, title=\"Data After Numbers Removal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences have been tokenized.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                      Data After Tokenization                                       </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                                                               </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"it\", \"is\", \"possible\", \"for\", \"him\", \"to\", \"finish\", \"the\", \"project\", \"on\", \"time\"] │ fear    │\n",
       "│ [\"maaari\", \"kang\", \"umalis\"]                                                           │ anger   │\n",
       "│ [\"i\", \"intend\", \"to\", \"go\", \"on\", \"a\", \"vacation\", \"next\", \"week\"]                     │ fear    │\n",
       "│ [\"he\", \"has\", \"already\", \"completed\", \"his\", \"homework\"]                               │ sadness │\n",
       "│ [\"maari\", \"ko\", \"syang\", \"puntahan\"]                                                   │ joy     │\n",
       "└────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                      Data After Tokenization                                       \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                                                              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"it\", \"is\", \"possible\", \"for\", \"him\", \"to\", \"finish\", \"the\", \"project\", \"on\", \"time\"] │ fear    │\n",
       "│ [\"maaari\", \"kang\", \"umalis\"]                                                           │ anger   │\n",
       "│ [\"i\", \"intend\", \"to\", \"go\", \"on\", \"a\", \"vacation\", \"next\", \"week\"]                     │ fear    │\n",
       "│ [\"he\", \"has\", \"already\", \"completed\", \"his\", \"homework\"]                               │ sadness │\n",
       "│ [\"maari\", \"ko\", \"syang\", \"puntahan\"]                                                   │ joy     │\n",
       "└────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_sentences(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda x: word_tokenize(x))\n",
    "        print(\"Sentences have been tokenized.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "tokenize_sentences(data)\n",
    "print_table(data, title=\"Data After Tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords have been removed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">               Data After Stopwords Removal               </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                     </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"possible\", \"finish\", \"project\", \"time\"]    │ fear    │\n",
       "│ [\"maaari\", \"kang\", \"umalis\"]                 │ anger   │\n",
       "│ [\"intend\", \"go\", \"vacation\", \"next\", \"week\"] │ fear    │\n",
       "│ [\"already\", \"completed\", \"homework\"]         │ sadness │\n",
       "│ [\"maari\", \"ko\", \"syang\", \"puntahan\"]         │ joy     │\n",
       "└──────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m               Data After Stopwords Removal               \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"possible\", \"finish\", \"project\", \"time\"]    │ fear    │\n",
       "│ [\"maaari\", \"kang\", \"umalis\"]                 │ anger   │\n",
       "│ [\"intend\", \"go\", \"vacation\", \"next\", \"week\"] │ fear    │\n",
       "│ [\"already\", \"completed\", \"homework\"]         │ sadness │\n",
       "│ [\"maari\", \"ko\", \"syang\", \"puntahan\"]         │ joy     │\n",
       "└──────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_stopwords(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        english_stopwords = set(stopwords.words('english'))\n",
    "        all_stopwords = english_stopwords.union(set(filipino_stopwords))\n",
    "        \n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: [word for word in tokens if word.lower() not in all_stopwords])\n",
    "        print(\"Stopwords have been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_stopwords(data)\n",
    "print_table(data, title=\"Data After Stopwords Removal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens have been lemmatized.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                 Data After Lemmatization                 </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                     </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"possible\", \"finish\", \"project\", \"time\"]    │ fear    │\n",
       "│ [\"maaari\", \"kang\", \"umalis\"]                 │ anger   │\n",
       "│ [\"intend\", \"go\", \"vacation\", \"next\", \"week\"] │ fear    │\n",
       "│ [\"already\", \"complete\", \"homework\"]          │ sadness │\n",
       "│ [\"maari\", \"ko\", \"syang\", \"puntahan\"]         │ joy     │\n",
       "└──────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                 Data After Lemmatization                 \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"possible\", \"finish\", \"project\", \"time\"]    │ fear    │\n",
       "│ [\"maaari\", \"kang\", \"umalis\"]                 │ anger   │\n",
       "│ [\"intend\", \"go\", \"vacation\", \"next\", \"week\"] │ fear    │\n",
       "│ [\"already\", \"complete\", \"homework\"]          │ sadness │\n",
       "│ [\"maari\", \"ko\", \"syang\", \"puntahan\"]         │ joy     │\n",
       "└──────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lemmatize_tokens(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: [nlp(token)[0].lemma_ for token in tokens])\n",
    "        print(\"Tokens have been lemmatized.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "lemmatize_tokens(data)\n",
    "print_table(data, title=\"Data After Lemmatization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens have been joined back into sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Data After Joining Tokens         </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                     </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ possible finish project time │ fear    │\n",
       "│ maaari kang umalis           │ anger   │\n",
       "│ intend go vacation next week │ fear    │\n",
       "│ already complete homework    │ sadness │\n",
       "│ maari ko syang puntahan      │ joy     │\n",
       "└──────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m        Data After Joining Tokens         \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ possible finish project time │ fear    │\n",
       "│ maaari kang umalis           │ anger   │\n",
       "│ intend go vacation next week │ fear    │\n",
       "│ already complete homework    │ sadness │\n",
       "│ maari ko syang puntahan      │ joy     │\n",
       "└──────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def join_tokens(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: ' '.join(tokens))\n",
    "        print(\"Tokens have been joined back into sentences.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "join_tokens(data)\n",
    "print_table(data, title=\"Data After Joining Tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorization complete.\n",
      "   already  balak  complete  finish        go  homework    intend     kang  \\\n",
      "0  0.00000    0.0   0.00000     0.5  0.000000   0.00000  0.000000  0.00000   \n",
      "1  0.00000    0.0   0.00000     0.0  0.000000   0.00000  0.000000  0.57735   \n",
      "2  0.00000    0.0   0.00000     0.0  0.447214   0.00000  0.447214  0.00000   \n",
      "3  0.57735    0.0   0.57735     0.0  0.000000   0.57735  0.000000  0.00000   \n",
      "4  0.00000    0.0   0.00000     0.0  0.000000   0.00000  0.000000  0.00000   \n",
      "\n",
      "   kanyang   ko  ...  puntahan  susunod  syang  takdangaralin  tapo  time  \\\n",
      "0      0.0  0.0  ...       0.0      0.0    0.0            0.0   0.0   0.5   \n",
      "1      0.0  0.0  ...       0.0      0.0    0.0            0.0   0.0   0.0   \n",
      "2      0.0  0.0  ...       0.0      0.0    0.0            0.0   0.0   0.0   \n",
      "3      0.0  0.0  ...       0.0      0.0    0.0            0.0   0.0   0.0   \n",
      "4      0.0  0.5  ...       0.5      0.0    0.5            0.0   0.0   0.0   \n",
      "\n",
      "    umalis  vacation      week  emotion  \n",
      "0  0.00000  0.000000  0.000000     fear  \n",
      "1  0.57735  0.000000  0.000000    anger  \n",
      "2  0.00000  0.447214  0.447214     fear  \n",
      "3  0.00000  0.000000  0.000000  sadness  \n",
      "4  0.00000  0.000000  0.000000      joy  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "def vectorize_with_tfidf(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "        tfidf_matrix = vectorizer.fit_transform(data['sentence'])\n",
    "\n",
    "        print(\"TF-IDF Vectorization complete.\")\n",
    "        return tfidf_matrix, vectorizer\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "tfidf_matrix, vectorizer = vectorize_with_tfidf(data)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "tfidf_df['emotion'] = data['emotion'].values\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
