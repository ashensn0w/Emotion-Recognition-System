{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Initial Setup\n",
    "\n",
    "The following libraries and modules are imported and initialized for text preprocessing and feature extraction:\n",
    "\n",
    "`pandas`: Library for data manipulation and analysis.\n",
    "\n",
    "`string`: Contains common string operations and constants.\n",
    "\n",
    "`nltk`: The Natural Language Toolkit, a comprehensive library for natural language processing. Here, it is used for:\n",
    "\n",
    "`nltk.tokenize.word_tokenize`: Tokenizes sentences into individual words (tokens).\n",
    "\n",
    "`nltk.corpus.stopwords`: Provides a set of common stopwords in multiple languages. These stopwords are used for filtering irrelevant words (e.g., \"the\", \"is\") from the text.\n",
    "\n",
    "`spacy`: Library for advanced natural language processing.\n",
    "\n",
    "`sklearn.feature_extraction.text.TfidfVectorizer`: For converting text to TF-IDF features.\n",
    "\n",
    "NLTK data (`punkt` and `stopwords`) is downloaded for tokenization and filtering out common stop words.\n",
    "\n",
    "The `en_core_web_trf` model from spaCy is loaded for processing text with transformers specifically for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filipino Stopwords\n",
    "\n",
    "The following list contains commonly used Filipino stopwords. These are words that are typically filtered out in text processing as they do not carry significant meaning in the context of text analysis.\n",
    "\n",
    "These stopwords include common words such as articles, prepositions, and conjunctions that are usually removed during text preprocessing to focus on more meaningful words in text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filipino_stopwords = [\n",
    "    \"a\", \"ako\", \"ang\", \"ano\", \"at\", \"ay\", \"ibang\", \"ito\", \"iyon\", \"ka\",\n",
    "    \"kami\", \"kanila\", \"kanya\", \"kayo\", \"laki\", \"mga\", \"na\", \"ng\", \"ni\",\n",
    "    \"nito\", \"nang\", \"sa\", \"sila\", \"tayo\", \"walang\", \"yung\", \"si\", \"bawat\",\n",
    "    \"kung\", \"hindi\", \"para\", \"dahil\", \"doon\", \"baka\", \"kapag\", \"saan\",\n",
    "    \"sino\", \"siya\", \"tama\", \"yan\", \"o\", \"pala\", \"pero\", \"wala\", \"huwag\",\n",
    "    \"muna\", \"na\", \"naman\", \"pag\", \"sana\", \"tulad\", \"upang\", \"bago\", \n",
    "    \"dati\", \"iba\", \"madami\", \"nakita\", \"pagkatapos\", \"pati\", \"sabi\", \"sana\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `load_dataset`\n",
    "\n",
    "The `load_dataset` function is used to load a dataset from a CSV file. It handles various potential errors during the file loading process.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please check the file path.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"File is empty. Please check the file content.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error parsing file. Please check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset and Extracting Sentences\n",
    "\n",
    "The following code snippet demonstrates how to load a dataset from a CSV file and extract sentences from it.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/training_data_fil_sample.csv'\n",
    "data = load_dataset(file_path)\n",
    "\n",
    "sentences = data['sentence'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `format_list_as_string`\n",
    "\n",
    "The `format_list_as_string` function converts a list of tokens into a formatted string representation.\n",
    "\n",
    "In text preprocessing, tokens are often stored as lists of words after steps like tokenization, stopword removal, and lemmatization. These lists can sometimes be difficult to read or display, especially when reviewing or debugging processed data.\n",
    "\n",
    "this function ensures that these lists are formatted neatly when printed.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_as_string(token_list):\n",
    "    return str(token_list).replace(\"'\", '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `print_table`\n",
    "\n",
    "The `print_table` function displays a formatted table using the `rich` library, which provides enhanced terminal output for data visualization.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                   Original Data                                                   </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                                                                              </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ \"Okay lang ako! Ano ba kayo?\"                                                                         │ anger   │\n",
       "│ \"Nasira ang iyong tibialis anterior.\"                                                                 │ fear    │\n",
       "│ \"Ha? Saan 'yun? Ay basta, ayos lang ako. Habulin na lang natin 'yung kriminal at baka makatakas       │ anger   │\n",
       "│ siya!\"                                                                                                │         │\n",
       "│ \"Nakakalungkot, nasa Kanluran siya.\"                                                                  │ sadness │\n",
       "│ \"Tsk. Kailangan kong magtago. Alam na ng mga pulis na ako ang pumatay sa hayup na 'yun,\" sabi nung    │ fear    │\n",
       "│ kriminal.                                                                                             │         │\n",
       "│ Ito na lang ang alaala ko sa kanya dahil . . . dahil . . . wala na rin si Demi na pusa niya.          │ sadness │\n",
       "│ Lagi kong sinisisi ang sarili ko dahil sa pagkamatay ni Demi at dahil napakahina ko noon.             │ sadness │\n",
       "│ Pero kapag naaalala ko 'yung sinabi nung Huntres habang nasa digmaan, lumalakas ang loob ko. Gusto    │ joy     │\n",
       "│ kong maging katulad niya. Kahit nasa gitna kami ng laban nun ay wala siyang pinakitang kahinaan,      │         │\n",
       "│ kahit na andaming namamatay sa paligid niya.                                                          │         │\n",
       "│ \"Sa burol na naman?\" sabay ngiti sa akin ni Akane.                                                    │ joy     │\n",
       "│ Napangiti tuloy ako bigla.                                                                            │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                   Original Data                                                   \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                                                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ \"Okay lang ako! Ano ba kayo?\"                                                                         │ anger   │\n",
       "│ \"Nasira ang iyong tibialis anterior.\"                                                                 │ fear    │\n",
       "│ \"Ha? Saan 'yun? Ay basta, ayos lang ako. Habulin na lang natin 'yung kriminal at baka makatakas       │ anger   │\n",
       "│ siya!\"                                                                                                │         │\n",
       "│ \"Nakakalungkot, nasa Kanluran siya.\"                                                                  │ sadness │\n",
       "│ \"Tsk. Kailangan kong magtago. Alam na ng mga pulis na ako ang pumatay sa hayup na 'yun,\" sabi nung    │ fear    │\n",
       "│ kriminal.                                                                                             │         │\n",
       "│ Ito na lang ang alaala ko sa kanya dahil . . . dahil . . . wala na rin si Demi na pusa niya.          │ sadness │\n",
       "│ Lagi kong sinisisi ang sarili ko dahil sa pagkamatay ni Demi at dahil napakahina ko noon.             │ sadness │\n",
       "│ Pero kapag naaalala ko 'yung sinabi nung Huntres habang nasa digmaan, lumalakas ang loob ko. Gusto    │ joy     │\n",
       "│ kong maging katulad niya. Kahit nasa gitna kami ng laban nun ay wala siyang pinakitang kahinaan,      │         │\n",
       "│ kahit na andaming namamatay sa paligid niya.                                                          │         │\n",
       "│ \"Sa burol na naman?\" sabay ngiti sa akin ni Akane.                                                    │ joy     │\n",
       "│ Napangiti tuloy ako bigla.                                                                            │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_table(data, title=\"Table\", num_samples=10):\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    \n",
    "    table = Table(title=title)\n",
    "    \n",
    "    for col in data.columns:\n",
    "        table.add_column(col)\n",
    "\n",
    "    for _, row in data.head(num_samples).iterrows():\n",
    "        formatted_row = [format_list_as_string(row[col]) if isinstance(row[col], list) else row[col] for col in data.columns]\n",
    "        table.add_row(*map(str, formatted_row))\n",
    "    \n",
    "    console = Console()\n",
    "    console.print(table)\n",
    "\n",
    "print_table(data, title=\"Original Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `convert_to_lowercase`\n",
    "\n",
    "The `convert_to_lowercase` function converts all text in the 'sentence' column of a DataFrame to lowercase.\n",
    "\n",
    "It ensures uniformity in text data by transforming all characters to lowercase, which is a key preprocessing step in natural language processing (NLP) tasks.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence has been converted to lowercase.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                          Data After Lowercase Conversion                                          </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                                                                              </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ \"okay lang ako! ano ba kayo?\"                                                                         │ anger   │\n",
       "│ \"nasira ang iyong tibialis anterior.\"                                                                 │ fear    │\n",
       "│ \"ha? saan 'yun? ay basta, ayos lang ako. habulin na lang natin 'yung kriminal at baka makatakas       │ anger   │\n",
       "│ siya!\"                                                                                                │         │\n",
       "│ \"nakakalungkot, nasa kanluran siya.\"                                                                  │ sadness │\n",
       "│ \"tsk. kailangan kong magtago. alam na ng mga pulis na ako ang pumatay sa hayup na 'yun,\" sabi nung    │ fear    │\n",
       "│ kriminal.                                                                                             │         │\n",
       "│ ito na lang ang alaala ko sa kanya dahil . . . dahil . . . wala na rin si demi na pusa niya.          │ sadness │\n",
       "│ lagi kong sinisisi ang sarili ko dahil sa pagkamatay ni demi at dahil napakahina ko noon.             │ sadness │\n",
       "│ pero kapag naaalala ko 'yung sinabi nung huntres habang nasa digmaan, lumalakas ang loob ko. gusto    │ joy     │\n",
       "│ kong maging katulad niya. kahit nasa gitna kami ng laban nun ay wala siyang pinakitang kahinaan,      │         │\n",
       "│ kahit na andaming namamatay sa paligid niya.                                                          │         │\n",
       "│ \"sa burol na naman?\" sabay ngiti sa akin ni akane.                                                    │ joy     │\n",
       "│ napangiti tuloy ako bigla.                                                                            │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                          Data After Lowercase Conversion                                          \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                                                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ \"okay lang ako! ano ba kayo?\"                                                                         │ anger   │\n",
       "│ \"nasira ang iyong tibialis anterior.\"                                                                 │ fear    │\n",
       "│ \"ha? saan 'yun? ay basta, ayos lang ako. habulin na lang natin 'yung kriminal at baka makatakas       │ anger   │\n",
       "│ siya!\"                                                                                                │         │\n",
       "│ \"nakakalungkot, nasa kanluran siya.\"                                                                  │ sadness │\n",
       "│ \"tsk. kailangan kong magtago. alam na ng mga pulis na ako ang pumatay sa hayup na 'yun,\" sabi nung    │ fear    │\n",
       "│ kriminal.                                                                                             │         │\n",
       "│ ito na lang ang alaala ko sa kanya dahil . . . dahil . . . wala na rin si demi na pusa niya.          │ sadness │\n",
       "│ lagi kong sinisisi ang sarili ko dahil sa pagkamatay ni demi at dahil napakahina ko noon.             │ sadness │\n",
       "│ pero kapag naaalala ko 'yung sinabi nung huntres habang nasa digmaan, lumalakas ang loob ko. gusto    │ joy     │\n",
       "│ kong maging katulad niya. kahit nasa gitna kami ng laban nun ay wala siyang pinakitang kahinaan,      │         │\n",
       "│ kahit na andaming namamatay sa paligid niya.                                                          │         │\n",
       "│ \"sa burol na naman?\" sabay ngiti sa akin ni akane.                                                    │ joy     │\n",
       "│ napangiti tuloy ako bigla.                                                                            │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_lowercase(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].str.lower()\n",
    "        print(\"Sentence has been converted to lowercase.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "convert_to_lowercase(data)\n",
    "print_table(data, title=\"Data After Lowercase Conversion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `remove_punctuation`\n",
    "\n",
    "The `remove_punctuation` function removes punctuation from all text in the 'sentence' column of a DataFrame.\n",
    "\n",
    "It ensures that only the meaningful words remain, free of any punctuation that could interfere with subsequent text processing steps.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation has been removed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                          Data After Punctuation Removal                                           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                                                                              </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ okay lang ako ano ba kayo                                                                             │ anger   │\n",
       "│ nasira ang iyong tibialis anterior                                                                    │ fear    │\n",
       "│ ha saan yun ay basta ayos lang ako habulin na lang natin yung kriminal at baka makatakas siya         │ anger   │\n",
       "│ nakakalungkot nasa kanluran siya                                                                      │ sadness │\n",
       "│ tsk kailangan kong magtago alam na ng mga pulis na ako ang pumatay sa hayup na yun sabi nung kriminal │ fear    │\n",
       "│ ito na lang ang alaala ko sa kanya dahil    dahil    wala na rin si demi na pusa niya                 │ sadness │\n",
       "│ lagi kong sinisisi ang sarili ko dahil sa pagkamatay ni demi at dahil napakahina ko noon              │ sadness │\n",
       "│ pero kapag naaalala ko yung sinabi nung huntres habang nasa digmaan lumalakas ang loob ko gusto kong  │ joy     │\n",
       "│ maging katulad niya kahit nasa gitna kami ng laban nun ay wala siyang pinakitang kahinaan kahit na    │         │\n",
       "│ andaming namamatay sa paligid niya                                                                    │         │\n",
       "│ sa burol na naman sabay ngiti sa akin ni akane                                                        │ joy     │\n",
       "│ napangiti tuloy ako bigla                                                                             │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                          Data After Punctuation Removal                                           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                                                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ okay lang ako ano ba kayo                                                                             │ anger   │\n",
       "│ nasira ang iyong tibialis anterior                                                                    │ fear    │\n",
       "│ ha saan yun ay basta ayos lang ako habulin na lang natin yung kriminal at baka makatakas siya         │ anger   │\n",
       "│ nakakalungkot nasa kanluran siya                                                                      │ sadness │\n",
       "│ tsk kailangan kong magtago alam na ng mga pulis na ako ang pumatay sa hayup na yun sabi nung kriminal │ fear    │\n",
       "│ ito na lang ang alaala ko sa kanya dahil    dahil    wala na rin si demi na pusa niya                 │ sadness │\n",
       "│ lagi kong sinisisi ang sarili ko dahil sa pagkamatay ni demi at dahil napakahina ko noon              │ sadness │\n",
       "│ pero kapag naaalala ko yung sinabi nung huntres habang nasa digmaan lumalakas ang loob ko gusto kong  │ joy     │\n",
       "│ maging katulad niya kahit nasa gitna kami ng laban nun ay wala siyang pinakitang kahinaan kahit na    │         │\n",
       "│ andaming namamatay sa paligid niya                                                                    │         │\n",
       "│ sa burol na naman sabay ngiti sa akin ni akane                                                        │ joy     │\n",
       "│ napangiti tuloy ako bigla                                                                             │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_punctuation(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "        print(\"Punctuation has been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_punctuation(data)\n",
    "print_table(data, title=\"Data After Punctuation Removal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `remove_numbers`\n",
    "\n",
    "The `remove_numbers` function removes numerical digits from all text in the 'sentence' column of a DataFrame.\n",
    "\n",
    "It ensures that the text data is free from irrelevant numerical characters that could distort the analysis.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers have been removed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                            Data After Numbers Removal                                             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                                                                              </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ okay lang ako ano ba kayo                                                                             │ anger   │\n",
       "│ nasira ang iyong tibialis anterior                                                                    │ fear    │\n",
       "│ ha saan yun ay basta ayos lang ako habulin na lang natin yung kriminal at baka makatakas siya         │ anger   │\n",
       "│ nakakalungkot nasa kanluran siya                                                                      │ sadness │\n",
       "│ tsk kailangan kong magtago alam na ng mga pulis na ako ang pumatay sa hayup na yun sabi nung kriminal │ fear    │\n",
       "│ ito na lang ang alaala ko sa kanya dahil    dahil    wala na rin si demi na pusa niya                 │ sadness │\n",
       "│ lagi kong sinisisi ang sarili ko dahil sa pagkamatay ni demi at dahil napakahina ko noon              │ sadness │\n",
       "│ pero kapag naaalala ko yung sinabi nung huntres habang nasa digmaan lumalakas ang loob ko gusto kong  │ joy     │\n",
       "│ maging katulad niya kahit nasa gitna kami ng laban nun ay wala siyang pinakitang kahinaan kahit na    │         │\n",
       "│ andaming namamatay sa paligid niya                                                                    │         │\n",
       "│ sa burol na naman sabay ngiti sa akin ni akane                                                        │ joy     │\n",
       "│ napangiti tuloy ako bigla                                                                             │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                            Data After Numbers Removal                                             \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                                                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ okay lang ako ano ba kayo                                                                             │ anger   │\n",
       "│ nasira ang iyong tibialis anterior                                                                    │ fear    │\n",
       "│ ha saan yun ay basta ayos lang ako habulin na lang natin yung kriminal at baka makatakas siya         │ anger   │\n",
       "│ nakakalungkot nasa kanluran siya                                                                      │ sadness │\n",
       "│ tsk kailangan kong magtago alam na ng mga pulis na ako ang pumatay sa hayup na yun sabi nung kriminal │ fear    │\n",
       "│ ito na lang ang alaala ko sa kanya dahil    dahil    wala na rin si demi na pusa niya                 │ sadness │\n",
       "│ lagi kong sinisisi ang sarili ko dahil sa pagkamatay ni demi at dahil napakahina ko noon              │ sadness │\n",
       "│ pero kapag naaalala ko yung sinabi nung huntres habang nasa digmaan lumalakas ang loob ko gusto kong  │ joy     │\n",
       "│ maging katulad niya kahit nasa gitna kami ng laban nun ay wala siyang pinakitang kahinaan kahit na    │         │\n",
       "│ andaming namamatay sa paligid niya                                                                    │         │\n",
       "│ sa burol na naman sabay ngiti sa akin ni akane                                                        │ joy     │\n",
       "│ napangiti tuloy ako bigla                                                                             │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_numbers(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].str.replace(r'\\d+', '', regex=True)\n",
    "        print(\"Numbers have been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_numbers(data)\n",
    "print_table(data, title=\"Data After Numbers Removal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `tokenize_sentences`\n",
    "\n",
    "The `tokenize_sentences` function tokenizes each sentence in the 'sentence' column of a DataFrame into individual words.\n",
    "\n",
    "It breaks down sentences into individual words or tokens, facilitating analysis, standardizing input, and enabling various natural language processing tasks.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences have been tokenized.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                              Data After Tokenization                                              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                                                                              </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"okay\", \"lang\", \"ako\", \"ano\", \"ba\", \"kayo\"]                                                          │ anger   │\n",
       "│ [\"nasira\", \"ang\", \"iyong\", \"tibialis\", \"anterior\"]                                                    │ fear    │\n",
       "│ [\"ha\", \"saan\", \"yun\", \"ay\", \"basta\", \"ayos\", \"lang\", \"ako\", \"habulin\", \"na\", \"lang\", \"natin\", \"yung\", │ anger   │\n",
       "│ \"kriminal\", \"at\", \"baka\", \"makatakas\", \"siya\"]                                                        │         │\n",
       "│ [\"nakakalungkot\", \"nasa\", \"kanluran\", \"siya\"]                                                         │ sadness │\n",
       "│ [\"tsk\", \"kailangan\", \"kong\", \"magtago\", \"alam\", \"na\", \"ng\", \"mga\", \"pulis\", \"na\", \"ako\", \"ang\",       │ fear    │\n",
       "│ \"pumatay\", \"sa\", \"hayup\", \"na\", \"yun\", \"sabi\", \"nung\", \"kriminal\"]                                    │         │\n",
       "│ [\"ito\", \"na\", \"lang\", \"ang\", \"alaala\", \"ko\", \"sa\", \"kanya\", \"dahil\", \"dahil\", \"wala\", \"na\", \"rin\",    │ sadness │\n",
       "│ \"si\", \"demi\", \"na\", \"pusa\", \"niya\"]                                                                   │         │\n",
       "│ [\"lagi\", \"kong\", \"sinisisi\", \"ang\", \"sarili\", \"ko\", \"dahil\", \"sa\", \"pagkamatay\", \"ni\", \"demi\", \"at\",  │ sadness │\n",
       "│ \"dahil\", \"napakahina\", \"ko\", \"noon\"]                                                                  │         │\n",
       "│ [\"pero\", \"kapag\", \"naaalala\", \"ko\", \"yung\", \"sinabi\", \"nung\", \"huntres\", \"habang\", \"nasa\", \"digmaan\", │ joy     │\n",
       "│ \"lumalakas\", \"ang\", \"loob\", \"ko\", \"gusto\", \"kong\", \"maging\", \"katulad\", \"niya\", \"kahit\", \"nasa\",      │         │\n",
       "│ \"gitna\", \"kami\", \"ng\", \"laban\", \"nun\", \"ay\", \"wala\", \"siyang\", \"pinakitang\", \"kahinaan\", \"kahit\",     │         │\n",
       "│ \"na\", \"andaming\", \"namamatay\", \"sa\", \"paligid\", \"niya\"]                                               │         │\n",
       "│ [\"sa\", \"burol\", \"na\", \"naman\", \"sabay\", \"ngiti\", \"sa\", \"akin\", \"ni\", \"akane\"]                         │ joy     │\n",
       "│ [\"napangiti\", \"tuloy\", \"ako\", \"bigla\"]                                                                │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                              Data After Tokenization                                              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                                                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"okay\", \"lang\", \"ako\", \"ano\", \"ba\", \"kayo\"]                                                          │ anger   │\n",
       "│ [\"nasira\", \"ang\", \"iyong\", \"tibialis\", \"anterior\"]                                                    │ fear    │\n",
       "│ [\"ha\", \"saan\", \"yun\", \"ay\", \"basta\", \"ayos\", \"lang\", \"ako\", \"habulin\", \"na\", \"lang\", \"natin\", \"yung\", │ anger   │\n",
       "│ \"kriminal\", \"at\", \"baka\", \"makatakas\", \"siya\"]                                                        │         │\n",
       "│ [\"nakakalungkot\", \"nasa\", \"kanluran\", \"siya\"]                                                         │ sadness │\n",
       "│ [\"tsk\", \"kailangan\", \"kong\", \"magtago\", \"alam\", \"na\", \"ng\", \"mga\", \"pulis\", \"na\", \"ako\", \"ang\",       │ fear    │\n",
       "│ \"pumatay\", \"sa\", \"hayup\", \"na\", \"yun\", \"sabi\", \"nung\", \"kriminal\"]                                    │         │\n",
       "│ [\"ito\", \"na\", \"lang\", \"ang\", \"alaala\", \"ko\", \"sa\", \"kanya\", \"dahil\", \"dahil\", \"wala\", \"na\", \"rin\",    │ sadness │\n",
       "│ \"si\", \"demi\", \"na\", \"pusa\", \"niya\"]                                                                   │         │\n",
       "│ [\"lagi\", \"kong\", \"sinisisi\", \"ang\", \"sarili\", \"ko\", \"dahil\", \"sa\", \"pagkamatay\", \"ni\", \"demi\", \"at\",  │ sadness │\n",
       "│ \"dahil\", \"napakahina\", \"ko\", \"noon\"]                                                                  │         │\n",
       "│ [\"pero\", \"kapag\", \"naaalala\", \"ko\", \"yung\", \"sinabi\", \"nung\", \"huntres\", \"habang\", \"nasa\", \"digmaan\", │ joy     │\n",
       "│ \"lumalakas\", \"ang\", \"loob\", \"ko\", \"gusto\", \"kong\", \"maging\", \"katulad\", \"niya\", \"kahit\", \"nasa\",      │         │\n",
       "│ \"gitna\", \"kami\", \"ng\", \"laban\", \"nun\", \"ay\", \"wala\", \"siyang\", \"pinakitang\", \"kahinaan\", \"kahit\",     │         │\n",
       "│ \"na\", \"andaming\", \"namamatay\", \"sa\", \"paligid\", \"niya\"]                                               │         │\n",
       "│ [\"sa\", \"burol\", \"na\", \"naman\", \"sabay\", \"ngiti\", \"sa\", \"akin\", \"ni\", \"akane\"]                         │ joy     │\n",
       "│ [\"napangiti\", \"tuloy\", \"ako\", \"bigla\"]                                                                │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_sentences(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda x: word_tokenize(x))\n",
    "        print(\"Sentences have been tokenized.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "tokenize_sentences(data)\n",
    "print_table(data, title=\"Data After Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `remove_stopwords`\n",
    "\n",
    "The `remove_stopwords` function removes stopwords from each tokenized sentence in the 'sentence' column of a DataFrame.\n",
    "\n",
    "Stopwords are common words that usually add little meaning to a sentence and are often filtered out in text processing to enhance the focus on more meaningful words.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords have been removed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                           Data After Stopwords Removal                                            </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                                                                              </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"okay\", \"lang\", \"ba\"]                                                                                │ anger   │\n",
       "│ [\"nasira\", \"iyong\", \"tibialis\", \"anterior\"]                                                           │ fear    │\n",
       "│ [\"ha\", \"yun\", \"basta\", \"ayos\", \"lang\", \"habulin\", \"lang\", \"natin\", \"kriminal\", \"makatakas\"]           │ anger   │\n",
       "│ [\"nakakalungkot\", \"nasa\", \"kanluran\"]                                                                 │ sadness │\n",
       "│ [\"tsk\", \"kailangan\", \"kong\", \"magtago\", \"alam\", \"pulis\", \"pumatay\", \"hayup\", \"yun\", \"nung\",           │ fear    │\n",
       "│ \"kriminal\"]                                                                                           │         │\n",
       "│ [\"lang\", \"alaala\", \"ko\", \"rin\", \"demi\", \"pusa\", \"niya\"]                                               │ sadness │\n",
       "│ [\"lagi\", \"kong\", \"sinisisi\", \"sarili\", \"ko\", \"pagkamatay\", \"demi\", \"napakahina\", \"ko\", \"noon\"]        │ sadness │\n",
       "│ [\"naaalala\", \"ko\", \"sinabi\", \"nung\", \"huntres\", \"habang\", \"nasa\", \"digmaan\", \"lumalakas\", \"loob\",     │ joy     │\n",
       "│ \"ko\", \"gusto\", \"kong\", \"maging\", \"katulad\", \"niya\", \"kahit\", \"nasa\", \"gitna\", \"laban\", \"nun\",         │         │\n",
       "│ \"siyang\", \"pinakitang\", \"kahinaan\", \"kahit\", \"andaming\", \"namamatay\", \"paligid\", \"niya\"]              │         │\n",
       "│ [\"burol\", \"sabay\", \"ngiti\", \"akin\", \"akane\"]                                                          │ joy     │\n",
       "│ [\"napangiti\", \"tuloy\", \"bigla\"]                                                                       │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                           Data After Stopwords Removal                                            \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                                                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"okay\", \"lang\", \"ba\"]                                                                                │ anger   │\n",
       "│ [\"nasira\", \"iyong\", \"tibialis\", \"anterior\"]                                                           │ fear    │\n",
       "│ [\"ha\", \"yun\", \"basta\", \"ayos\", \"lang\", \"habulin\", \"lang\", \"natin\", \"kriminal\", \"makatakas\"]           │ anger   │\n",
       "│ [\"nakakalungkot\", \"nasa\", \"kanluran\"]                                                                 │ sadness │\n",
       "│ [\"tsk\", \"kailangan\", \"kong\", \"magtago\", \"alam\", \"pulis\", \"pumatay\", \"hayup\", \"yun\", \"nung\",           │ fear    │\n",
       "│ \"kriminal\"]                                                                                           │         │\n",
       "│ [\"lang\", \"alaala\", \"ko\", \"rin\", \"demi\", \"pusa\", \"niya\"]                                               │ sadness │\n",
       "│ [\"lagi\", \"kong\", \"sinisisi\", \"sarili\", \"ko\", \"pagkamatay\", \"demi\", \"napakahina\", \"ko\", \"noon\"]        │ sadness │\n",
       "│ [\"naaalala\", \"ko\", \"sinabi\", \"nung\", \"huntres\", \"habang\", \"nasa\", \"digmaan\", \"lumalakas\", \"loob\",     │ joy     │\n",
       "│ \"ko\", \"gusto\", \"kong\", \"maging\", \"katulad\", \"niya\", \"kahit\", \"nasa\", \"gitna\", \"laban\", \"nun\",         │         │\n",
       "│ \"siyang\", \"pinakitang\", \"kahinaan\", \"kahit\", \"andaming\", \"namamatay\", \"paligid\", \"niya\"]              │         │\n",
       "│ [\"burol\", \"sabay\", \"ngiti\", \"akin\", \"akane\"]                                                          │ joy     │\n",
       "│ [\"napangiti\", \"tuloy\", \"bigla\"]                                                                       │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_stopwords(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        english_stopwords = set(stopwords.words('english'))\n",
    "        all_stopwords = english_stopwords.union(set(filipino_stopwords))\n",
    "        \n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: [word for word in tokens if word.lower() not in all_stopwords])\n",
    "        print(\"Stopwords have been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_stopwords(data)\n",
    "print_table(data, title=\"Data After Stopwords Removal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `lemmatize_tokens`\n",
    "\n",
    "The `lemmatize_tokens` function lemmatizes each token in the 'sentence' column of a DataFrame using spaCy's lemmatization.\n",
    "\n",
    "it reduces words to their base or dictionary form (lemmas), which helps in standardizing the text by consolidating different inflections of a word into a single representation, thereby improving the performance of natural language processing tasks such as classification and clustering.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens have been lemmatized.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                             Data After Lemmatization                                              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                                                                              </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"okay\", \"lang\", \"ba\"]                                                                                │ anger   │\n",
       "│ [\"nasira\", \"iyong\", \"tibialis\", \"anterior\"]                                                           │ fear    │\n",
       "│ [\"ha\", \"yun\", \"basta\", \"ayos\", \"lang\", \"habulin\", \"lang\", \"natin\", \"kriminal\", \"makatakas\"]           │ anger   │\n",
       "│ [\"nakakalungkot\", \"nasa\", \"kanluran\"]                                                                 │ sadness │\n",
       "│ [\"tsk\", \"kailangan\", \"kong\", \"magtago\", \"alam\", \"pulis\", \"pumatay\", \"hayup\", \"yun\", \"nung\",           │ fear    │\n",
       "│ \"kriminal\"]                                                                                           │         │\n",
       "│ [\"lang\", \"alaala\", \"ko\", \"rin\", \"demi\", \"pusa\", \"niya\"]                                               │ sadness │\n",
       "│ [\"lagi\", \"kong\", \"sinisisi\", \"sarili\", \"ko\", \"pagkamatay\", \"demi\", \"napakahina\", \"ko\", \"noon\"]        │ sadness │\n",
       "│ [\"naaalala\", \"ko\", \"sinabi\", \"nung\", \"huntres\", \"habang\", \"nasa\", \"digmaan\", \"lumalakas\", \"loob\",     │ joy     │\n",
       "│ \"ko\", \"gusto\", \"kong\", \"mage\", \"katulad\", \"niya\", \"kahit\", \"nasa\", \"gitna\", \"laban\", \"nun\", \"siyang\", │         │\n",
       "│ \"pinakitang\", \"kahinaan\", \"kahit\", \"andame\", \"namamatay\", \"paligid\", \"niya\"]                          │         │\n",
       "│ [\"burol\", \"sabay\", \"ngiti\", \"akin\", \"akane\"]                                                          │ joy     │\n",
       "│ [\"napangiti\", \"tuloy\", \"bigla\"]                                                                       │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                             Data After Lemmatization                                              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                                                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ [\"okay\", \"lang\", \"ba\"]                                                                                │ anger   │\n",
       "│ [\"nasira\", \"iyong\", \"tibialis\", \"anterior\"]                                                           │ fear    │\n",
       "│ [\"ha\", \"yun\", \"basta\", \"ayos\", \"lang\", \"habulin\", \"lang\", \"natin\", \"kriminal\", \"makatakas\"]           │ anger   │\n",
       "│ [\"nakakalungkot\", \"nasa\", \"kanluran\"]                                                                 │ sadness │\n",
       "│ [\"tsk\", \"kailangan\", \"kong\", \"magtago\", \"alam\", \"pulis\", \"pumatay\", \"hayup\", \"yun\", \"nung\",           │ fear    │\n",
       "│ \"kriminal\"]                                                                                           │         │\n",
       "│ [\"lang\", \"alaala\", \"ko\", \"rin\", \"demi\", \"pusa\", \"niya\"]                                               │ sadness │\n",
       "│ [\"lagi\", \"kong\", \"sinisisi\", \"sarili\", \"ko\", \"pagkamatay\", \"demi\", \"napakahina\", \"ko\", \"noon\"]        │ sadness │\n",
       "│ [\"naaalala\", \"ko\", \"sinabi\", \"nung\", \"huntres\", \"habang\", \"nasa\", \"digmaan\", \"lumalakas\", \"loob\",     │ joy     │\n",
       "│ \"ko\", \"gusto\", \"kong\", \"mage\", \"katulad\", \"niya\", \"kahit\", \"nasa\", \"gitna\", \"laban\", \"nun\", \"siyang\", │         │\n",
       "│ \"pinakitang\", \"kahinaan\", \"kahit\", \"andame\", \"namamatay\", \"paligid\", \"niya\"]                          │         │\n",
       "│ [\"burol\", \"sabay\", \"ngiti\", \"akin\", \"akane\"]                                                          │ joy     │\n",
       "│ [\"napangiti\", \"tuloy\", \"bigla\"]                                                                       │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lemmatize_tokens(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: [nlp(token)[0].lemma_ for token in tokens])\n",
    "        print(\"Tokens have been lemmatized.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "lemmatize_tokens(data)\n",
    "print_table(data, title=\"Data After Lemmatization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `join_tokens`\n",
    "\n",
    "The `join_tokens` function joins tokens in each list within the 'sentence' column of a DataFrame back into single sentences.\n",
    "\n",
    "it reconstructs the original text format after various transformations, enabling further analysis or modeling tasks that require complete sentences, such as text classification, sentiment analysis, or readability assessments.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens have been joined back into sentences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                             Data After Joining Tokens                                             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> sentence                                                                                              </span>┃<span style=\"font-weight: bold\"> emotion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ okay lang ba                                                                                          │ anger   │\n",
       "│ nasira iyong tibialis anterior                                                                        │ fear    │\n",
       "│ ha yun basta ayos lang habulin lang natin kriminal makatakas                                          │ anger   │\n",
       "│ nakakalungkot nasa kanluran                                                                           │ sadness │\n",
       "│ tsk kailangan kong magtago alam pulis pumatay hayup yun nung kriminal                                 │ fear    │\n",
       "│ lang alaala ko rin demi pusa niya                                                                     │ sadness │\n",
       "│ lagi kong sinisisi sarili ko pagkamatay demi napakahina ko noon                                       │ sadness │\n",
       "│ naaalala ko sinabi nung huntres habang nasa digmaan lumalakas loob ko gusto kong mage katulad niya    │ joy     │\n",
       "│ kahit nasa gitna laban nun siyang pinakitang kahinaan kahit andame namamatay paligid niya             │         │\n",
       "│ burol sabay ngiti akin akane                                                                          │ joy     │\n",
       "│ napangiti tuloy bigla                                                                                 │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                             Data After Joining Tokens                                             \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1msentence                                                                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1memotion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ okay lang ba                                                                                          │ anger   │\n",
       "│ nasira iyong tibialis anterior                                                                        │ fear    │\n",
       "│ ha yun basta ayos lang habulin lang natin kriminal makatakas                                          │ anger   │\n",
       "│ nakakalungkot nasa kanluran                                                                           │ sadness │\n",
       "│ tsk kailangan kong magtago alam pulis pumatay hayup yun nung kriminal                                 │ fear    │\n",
       "│ lang alaala ko rin demi pusa niya                                                                     │ sadness │\n",
       "│ lagi kong sinisisi sarili ko pagkamatay demi napakahina ko noon                                       │ sadness │\n",
       "│ naaalala ko sinabi nung huntres habang nasa digmaan lumalakas loob ko gusto kong mage katulad niya    │ joy     │\n",
       "│ kahit nasa gitna laban nun siyang pinakitang kahinaan kahit andame namamatay paligid niya             │         │\n",
       "│ burol sabay ngiti akin akane                                                                          │ joy     │\n",
       "│ napangiti tuloy bigla                                                                                 │ joy     │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def join_tokens(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: ' '.join(tokens))\n",
    "        print(\"Tokens have been joined back into sentences.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "join_tokens(data)\n",
    "print_table(data, title=\"Data After Joining Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `vectorize_with_tfidf`\n",
    "\n",
    "The `vectorize_with_tfidf` function performs TF-IDF vectorization on the 'sentence' column of a DataFrame.\n",
    "\n",
    "**TF-IDF** is a statistical measure used to evaluate the importance of a word in a document relative to a collection (or corpus) of documents. It combines two key concepts:\n",
    "\n",
    "1. **Term Frequency (TF)**: Measures how frequently a word appears in a single document. Words that appear frequently within a document are considered more important for that document.\n",
    "   - Formula: `TF = (Number of times a term appears in a document) / (Total number of terms in the document)`\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: Measures how important a word is across the entire dataset. Words that appear in many documents (common words) are less important, while rare words are more important.\n",
    "   - Formula: `IDF = log(Total number of documents / Number of documents containing the term)`\n",
    "\n",
    "The **TF-IDF score** is calculated by multiplying TF and IDF for each word, providing a numerical value representing the word's significance within a document relative to the entire corpus.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorization complete.\n",
      "   akane  akin  alaala      alam  andame  anterior      ayos       ba  \\\n",
      "0    0.0   0.0     0.0  0.000000     0.0       0.0  0.000000  0.62584   \n",
      "1    0.0   0.0     0.0  0.000000     0.0       0.5  0.000000  0.00000   \n",
      "2    0.0   0.0     0.0  0.000000     0.0       0.0  0.321781  0.00000   \n",
      "3    0.0   0.0     0.0  0.000000     0.0       0.0  0.000000  0.00000   \n",
      "4    0.0   0.0     0.0  0.320732     0.0       0.0  0.000000  0.00000   \n",
      "\n",
      "      basta  bigla  ...  sabay  sarili  sinabi  sinisisi  siyang  tibialis  \\\n",
      "0  0.000000    0.0  ...    0.0     0.0     0.0       0.0     0.0       0.0   \n",
      "1  0.000000    0.0  ...    0.0     0.0     0.0       0.0     0.0       0.5   \n",
      "2  0.321781    0.0  ...    0.0     0.0     0.0       0.0     0.0       0.0   \n",
      "3  0.000000    0.0  ...    0.0     0.0     0.0       0.0     0.0       0.0   \n",
      "4  0.000000    0.0  ...    0.0     0.0     0.0       0.0     0.0       0.0   \n",
      "\n",
      "        tsk  tuloy       yun  emotion  \n",
      "0  0.000000    0.0  0.000000    anger  \n",
      "1  0.000000    0.0  0.000000     fear  \n",
      "2  0.000000    0.0  0.273543    anger  \n",
      "3  0.000000    0.0  0.000000  sadness  \n",
      "4  0.320732    0.0  0.272652     fear  \n",
      "\n",
      "[5 rows x 68 columns]\n"
     ]
    }
   ],
   "source": [
    "def vectorize_with_tfidf(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "        tfidf_matrix = vectorizer.fit_transform(data['sentence'])\n",
    "\n",
    "        print(\"TF-IDF Vectorization complete.\")\n",
    "        return tfidf_matrix, vectorizer\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "tfidf_matrix, vectorizer = vectorize_with_tfidf(data)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "tfidf_df['emotion'] = data['emotion'].values\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
