{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "\n",
    "The `import spacy` A library for Natural Language Processing (NLP) that supports various language models.\n",
    "\n",
    "The `import pandas as pd` statement imports the pandas library, a powerful tool for data analysis and manipulation in Python. It is used here to load the dataset from a CSV file, allowing for better handling and organization of the data.\n",
    "\n",
    "The `nlp = spacy.load(\"xx_ent_wiki_sm\")`: This line loads the pre-trained SpaCy model xx_ent_wiki_sm. This model supports multiple languages, including Filipino, which is used here primarily for dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Mode Feature\n",
    "\n",
    "**Mode** - Expression of possibility, impossibility, necessity, or prohibition of an action.\n",
    "\n",
    "The `extract_mode` function analyzes a given Filipino text to detect specific modes such as Possibility, Impossibility, Necessity, and Prohibition based on defined keywords and their grammatical relationships. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mode(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        mode_keywords = json.load(f)['mode']\n",
    "\n",
    "    detected_modes = {\n",
    "        'Possibility': False,\n",
    "        'Impossibility': False,\n",
    "        'Necessity': False,\n",
    "        'Prohibition': False\n",
    "    }\n",
    "\n",
    "    for token in doc:\n",
    "        for mode, keywords in mode_keywords.items():\n",
    "            if token.text.lower() in keywords:\n",
    "                detected_modes[mode] = True\n",
    "\n",
    "        if token.dep_ in ['acomp', 'xcomp', 'ccomp']:\n",
    "            for mode, keywords in mode_keywords.items():\n",
    "                if token.head.text.lower() in keywords:\n",
    "                    detected_modes[mode] = True\n",
    "\n",
    "    return 1 if any(detected_modes.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Intention Feature\n",
    "\n",
    "**Intention** - The intention to perform an action.\n",
    "\n",
    "The `extract_intention` function identifies whether a sentence expresses intention. It checks for specific verbs, infinitive Verbs, modal Verbs, purpose clauses, or auxiliary verbs signaling future actions. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 20,
=======
   "execution_count": 3,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intention(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        intention_keywords = json.load(f)['intention']\n",
    "    \n",
    "    detected_intentions = {\n",
    "        'Infinitive Verbs': False,\n",
    "        'Modal Verbs': False,\n",
    "        'Purpose Clauses': False,\n",
    "        'Auxiliary Verbs': False\n",
    "    }\n",
<<<<<<< Updated upstream
    "\n",
=======
    "    \n",
>>>>>>> Stashed changes
    "    for token in doc:\n",
    "        if token.text.lower() in intention_keywords['Infinitive Verbs']:\n",
    "            detected_intentions['Infinitive Verbs'] = True\n",
    "        elif token.text.lower() in intention_keywords['Modal Verbs']:\n",
    "            detected_intentions['Modal Verbs'] = True\n",
    "        elif token.text.lower() in intention_keywords['Purpose Clauses']:\n",
    "            detected_intentions['Purpose Clauses'] = True\n",
    "        elif token.text.lower() in intention_keywords['Auxiliary Verbs']:\n",
    "            detected_intentions['Auxiliary Verbs'] = True\n",
    "        \n",
    "        if token.dep_ in ['acl', 'amod', 'xcomp'] and token.head.text.lower() in intention_keywords['Modal Verbs']:\n",
    "            detected_intentions['Modal Verbs'] = True\n",
    "        elif token.dep_ in ['acl', 'xcomp'] and token.text.lower() in intention_keywords['Infinitive Verbs']:\n",
    "            detected_intentions['Infinitive Verbs'] = True\n",
    "\n",
    "    return 1 if any(detected_intentions.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Result Feature\n",
    "\n",
    "**Result** - Action presented as already accomplished.\n",
    "\n",
    "The `extract_result` function identifies whether a sentence expresses a result. It checks for perfect tenses, resultative constructions, and the presence of the phrase \"as a result.\" The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_result(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        result_keywords = json.load(f)['result']\n",
=======
    "    result_keywords = {\n",
    "        'Completed Actions': ['natapos', 'nagawa', 'nakuha','nagresulta', 'humantong', 'nagdulot', \n",
    "        'nagbunga', 'naghatid', 'nagbigay-daan', 'naging', 'naging sanhi', \n",
    "        'nagbunga', 'natamo', 'nakuha', 'nakamit', 'nagtagumpay', \n",
    "        'nakasama', 'nakatulong', 'naranasan', 'nakapagbigay', 'nakabuo', \n",
    "        'nakapagdulot', 'napunta', 'napatunayan', 'naabot', 'nakamtan', \n",
    "        'natupad', 'nagbunga', 'nag-dulot', ],\n",
    "\n",
    "        'Perfect Aspect Verbs': ['nagkaroon', 'nagawa', 'umunlad', 'sumulong', 'nag-asenso', \n",
    "        'naglaho', 'naganap', 'nangyari', 'nasaksihan', 'naipakita', \n",
    "        'naipamalas', 'napagtagumpayan', 'naibalik', 'naipasa', 'naiwasan', \n",
    "        'naabot', 'nadama', 'nalaman', 'naramdaman', \n",
    "        'nakapagbago', 'napagpasyahan']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_results = {\n",
    "        'Completed Actions': False,\n",
    "        'Perfect Aspect Verbs': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in result_keywords['Completed Actions']:\n",
    "            detected_results['Completed Actions'] = True\n",
    "        elif token.text.lower() in result_keywords['Perfect Aspect Verbs']:\n",
    "            detected_results['Perfect Aspect Verbs'] = True\n",
    "        \n",
    "        if token.dep_ in ['attr', 'ccomp', 'acomp'] and token.head.text.lower() in result_keywords['Completed Actions']:\n",
    "            detected_results['Completed Actions'] = True\n",
    "        elif token.dep_ in ['attr', 'xcomp'] and token.text.lower() in result_keywords['Perfect Aspect Verbs']:\n",
    "            detected_results['Perfect Aspect Verbs'] = True\n",
    "\n",
    "    return 1 if any(detected_results.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Manner Feature\n",
    "\n",
    "**Manner** - Specification of the manner in which an action occurs, or expression of its intensity.\n",
    "\n",
    "The `extract_manner` function identifies whether a sentence describes the manner in which an action is performed. It checks for adverbs used as adverbial modifiers, prepositional phrases indicating manner, and Adjectives modifying other adverbs. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_manner(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        manner_keywords = json.load(f)['manner']\n",
=======
    "    manner_keywords = {\n",
    "        'Adverbs': ['maingat', 'maayos', 'mabilis', 'tahimik', 'malumanay', \n",
    "        'mahinahon', 'masinsin', 'magaan', 'mabagal', 'matapang', \n",
    "        'malakas', 'masigla', 'puspusan', 'paggalang', 'matiyaga', \n",
    "        'tapat', 'malasakit', 'mabait', 'matapang', 'mahinahon','masinsinan', 'mapanuri', 'masusing'],\n",
    "\n",
    "        'Adjectives as Adverbs': ['maganda','masikap', 'pagsisikap', 'masigasig', 'masinop', 'masipag', \n",
    "        'kakayahan', 'tiwala', 'kalooban', 'pag-asa', 'pagmamahal', 'respeto', 'malakas', 'maisip', \n",
    "        'pagkilala', 'pagsasaalang-alang', 'madamdamin', 'pagsusumikap', 'pagnanais', 'kasiglahan', 'kalakasan', \n",
    "        'kasipagan', 'pangarap',]\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_manners = {\n",
    "        'Adverbs': False,\n",
    "        'Adjectives as Adverbs': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in manner_keywords['Adverbs']:\n",
    "            detected_manners['Adverbs'] = True\n",
    "        elif token.text.lower() in manner_keywords['Adjectives as Adverbs']:\n",
    "            detected_manners['Adjectives as Adverbs'] = True\n",
    "        \n",
    "        if token.dep_ in ['advmod'] and token.text.lower() in manner_keywords['Adverbs']:\n",
    "            detected_manners['Adverbs'] = True\n",
    "        elif token.dep_ in ['amod'] and token.text.lower() in manner_keywords['Adjectives as Adverbs']:\n",
    "            detected_manners['Adjectives as Adverbs'] = True\n",
    "\n",
    "    return 1 if any(detected_manners.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Aspect Feature\n",
    "\n",
    "**Aspect** - Expression of the temporal contour of an action.\n",
    "\n",
    "The `extract_aspect` function identifies whether a sentence expresses an aspect of an action, such as its completion, ongoing nature, or habituality. It checks for aspectual verbs, auxiliary verbs with past participles or gerunds, and adverbs related to aspect. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aspect(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        aspect_keywords = json.load(f)['aspect']\n",
=======
    "    aspect_keywords = {\n",
    "        'Aspectual Markers': ['nag', 'naka', 'nagsa'],\n",
    "        \n",
    "        'Verbal Affixes': ['nag-aaral', 'natapos','patuloy','nagpatuloy', 'patuloy na nangyayari', \n",
    "        'nangyayari', 'nagpapatuloy', 'nagsimula', 'nagwakas', \n",
    "        'nagsisimula', 'natatapos', 'nagaganap', 'patuloy na', 'nangyayari pa', \n",
    "        'nagsisimula pa lang', 'nagsimula na', 'patuloy na nagaganap', \n",
    "        'nagtatapos', 'nagsimula', 'natapos na', ]\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_aspects = {\n",
    "        'Aspectual Markers': False,\n",
    "        'Verbal Affixes': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in aspect_keywords['Aspectual Markers']:\n",
    "            detected_aspects['Aspectual Markers'] = True\n",
    "        elif token.text.lower() in aspect_keywords['Verbal Affixes']:\n",
    "            detected_aspects['Verbal Affixes'] = True\n",
    "        \n",
    "        if token.dep_ in ['acl', 'amod'] and any(affix in token.text.lower() for affix in aspect_keywords['Aspectual Markers']):\n",
    "            detected_aspects['Aspectual Markers'] = True\n",
    "        elif token.dep_ in ['acl', 'xcomp'] and any(affix in token.text.lower() for affix in aspect_keywords['Verbal Affixes']):\n",
    "            detected_aspects['Verbal Affixes'] = True\n",
    "\n",
    "    return 1 if any(detected_aspects.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Status Feature\n",
    "\n",
    "**Status** - Negation of the action.\n",
    "\n",
    "The `extract_status` function identifies whether a sentence expresses a status, particularly negation. It checks for multi-word negation phrases, single-word negations, and dependency-based negations. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_status(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        status_keywords = json.load(f)['status']\n",
=======
    "    status_keywords = {\n",
    "        'Negation Words': ['hindi', 'wala', 'huwag']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_status = {\n",
    "        'Negation Words': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in status_keywords['Negation Words']:\n",
    "            detected_status['Negation Words'] = True\n",
    "        \n",
    "        if token.dep_ in ['neg'] and token.text.lower() in status_keywords['Negation Words']:\n",
    "            detected_status['Negation Words'] = True\n",
    "\n",
    "    return 1 if any(detected_status.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Appearance Feature\n",
    "\n",
    "**Appearance** - Indication of the replacement of one event by another.\n",
    "\n",
    "The `extract_appearance` function detects whether a sentence describes a change in appearance. It looks for transformational verbs, and negations related to change. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_appearance(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        appearance_keywords = json.load(f)['appearance']\n",
=======
    "    appearance_keywords = {\n",
    "        'Transition Words': ['naging', 'pinalitan', 'nagbago','nagpakita', \n",
    "        'nagsilbing', 'nagmumungkahi', 'nagpamalas', 'nagpahayag', \n",
    "        'nagbubukas', 'nagbibigay', 'nagsasalita', 'nag-aalok', 'naglalaman', 'nagsasabi', \n",
    "        'nagsusumpa', 'nag-aangkin', 'nagpapakita ng', 'nagpapahayag ng','naglalantad ng', \n",
    "        'nagsasalita ng', 'nag-aalok ng', 'naglalaman ng','nagpapatunay ng']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_appearance = {\n",
    "        'Transition Words': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in appearance_keywords['Transition Words']:\n",
    "            detected_appearance['Transition Words'] = True\n",
    "        \n",
    "        if token.dep_ in ['ccomp', 'acl'] and token.text.lower() in appearance_keywords['Transition Words']:\n",
    "            detected_appearance['Transition Words'] = True\n",
    "\n",
    "    return 1 if any(detected_appearance.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Knowledge Feature\n",
    "\n",
    "**Knowledge** - Description of awareness of the action.\n",
    "\n",
    "The `extract_knowledge` function determines if a sentence contains expressions of knowledge or awareness. It checks for cognitive verbs, perception verbs, reporting verbs, and specific markers like \"that.\" The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_knowledge(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        knowledge_keywords = json.load(f)['knowledge']\n",
=======
    "    knowledge_keywords = {\n",
    "        'Knowledge Verbs': ['alam', 'nauunawaan','nalaman','nalalaman', \n",
    "        'napagtanto', 'natutunan', 'kilala', 'nalaman', 'nasusundan', 'nauunawaan', \n",
    "        'nagkakaroon','nagtuturo', 'nagbibigay ng kaalaman', 'nagtuturo ng', 'nagpapaliwanag ng', 'nagsasalita ng', \n",
    "        'nagbibigay ng impormasyon', 'nagpapahayag ng', 'nagsusuri ng', 'nagtuturo ng', \n",
    "        'nagbibigay-diin', 'nagpapahayag ng','nagbibigay-alam']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_knowledge = {\n",
    "        'Knowledge Verbs': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in knowledge_keywords['Knowledge Verbs']:\n",
    "            detected_knowledge['Knowledge Verbs'] = True\n",
    "        \n",
    "        if token.dep_ in ['ccomp', 'acl'] and token.text.lower() in knowledge_keywords['Knowledge Verbs']:\n",
    "            detected_knowledge['Knowledge Verbs'] = True\n",
    "\n",
    "    return 1 if any(detected_knowledge.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Description Feature\n",
    "\n",
    "**Description** - Description of an act of communication.\n",
    "\n",
    "The `extract_description` function identifies whether a sentence contains expressions of description or communication. It checks for reporting verbs, complement clauses, direct speech, and specific modifier relations. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_description(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        description_keywords = json.load(f)['description']\n",
=======
    "    description_keywords = {\n",
    "        'Descriptive Phrases': ['sinabi', 'nasabi', 'sinasabi','naglarawan','inilarawan', \n",
    "        'nagsalaysay', 'nagdetalye','nagpaliwanag', 'nagpapakita', \n",
    "        'nagpahayag', 'nagbibigay', 'nagpapaliwanag ', 'nagbigay', \n",
    "        'nagbibigay-diin', 'nagpapahayag', 'nagpapaliwanag']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_descriptions = {\n",
    "        'Descriptive Phrases': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in description_keywords['Descriptive Phrases']:\n",
    "            detected_descriptions['Descriptive Phrases'] = True\n",
    "        \n",
    "        if token.dep_ in ['amod', 'acomp'] and token.text.lower() in description_keywords['Descriptive Phrases']:\n",
    "            detected_descriptions['Descriptive Phrases'] = True\n",
    "\n",
    "    return 1 if any(detected_descriptions.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Supposition Feature\n",
    "\n",
    "**Supposition** - Description of anticipation of a future action.\n",
    "\n",
    "The `extract_supposition` function identifies whether a sentence contains elements that suggest supposition or prediction. It checks for modal verbs, verbs related to expectation, adverbs indicating probability, and relevant dependency labels. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_supposition(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        supposition_keywords = json.load(f)['supposition']\n",
=======
    "    supposition_keywords = {\n",
    "        'Supposition Modal Verbs': ['maaaring', 'baka', 'sana','akala', 'pagpapalagay', 'kumpiyansa', 'hinuha', \n",
    "        'palagay', 'imahinasyon', 'halimbawa', 'sabi', 'tulad', \n",
    "        'sakaling', 'halimbawang', 'nagpapalagay', 'akalang', \n",
    "        'nagpapalagay', 'nag-aakala ', 'nag-iisip', \n",
    "        'nagpapalagay', 'nag-aakalang','sakali']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_suppositions = {\n",
    "        'Supposition Modal Verbs': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in supposition_keywords['Supposition Modal Verbs']:\n",
    "            detected_suppositions['Supposition Modal Verbs'] = True\n",
    "        \n",
    "        if token.dep_ in ['xcomp', 'acl'] and token.text.lower() in supposition_keywords['Supposition Modal Verbs']:\n",
    "            detected_suppositions['Supposition Modal Verbs'] = True\n",
    "\n",
    "    return 1 if any(detected_suppositions.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Subjectivation Feature\n",
    "\n",
    "**Subjectivation** - Attribution of the action, as an object of observation, to a subject.\n",
    "\n",
    "The `extract_subjectivation` function identifies whether a sentence contains elements that suggest subjectivation, indicating that the subject is involved in or perceiving the action. It checks for cognitive verbs, subject-verb agreement, and subjective adjectives. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subjectivation(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        subjectivation_keywords = json.load(f)['subjectivation']\n",
=======
    "    subjectivation_keywords = {\n",
    "        'Perception Verbs': ['nagbigay','nagpakita', 'nagsalaysay', 'naglarawan', \n",
    "        'nagsalita', 'nagsabi', 'naikwento', 'nagsasalaysay ', 'nagbigay', \n",
    "        'nagsasabi','nakikita', 'nararamdaman', 'iniisip']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_subjectivation = {\n",
    "        'Perception Verbs': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in subjectivation_keywords['Perception Verbs']:\n",
    "            detected_subjectivation['Perception Verbs'] = True\n",
    "        \n",
    "        if token.dep_ in ['ccomp', 'xcomp'] and token.text.lower() in subjectivation_keywords['Perception Verbs']:\n",
    "            detected_subjectivation['Perception Verbs'] = True\n",
    "\n",
    "    return 1 if any(detected_subjectivation.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Attitude Feature\n",
    "\n",
    "**Attitude** - Description of the state elicited in the subject by the action.\n",
    "\n",
    "The `extract_attitude` function identifies whether a sentence contains elements that suggest the subject's attitude or emotional state. It checks for emotion-related  adjectives. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attitude(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        attitude_keywords = json.load(f)['attitude']\n",
=======
    "    attitude_keywords = {\n",
    "        'Emotion-related Adjectives': ['masaya', 'nalungkot', 'nagulat',\n",
    "        'nagustuhan', 'hindi nagustuhan', 'pabor', 'hindi pabor', \n",
    "        'sumasang-ayon', 'hindi sumasang-ayon', 'natuwa', 'nainis', \n",
    "        'nagalit', 'nagagalit', 'malungkot', 'nakakaawa', \n",
    "        'nag-aalala', 'natuwa', 'nabahala', 'nag-alala', 'nagagalit', \n",
    "        'nag-iba ng pananaw', 'nagiging positibo', 'nagiging negatibo', \n",
    "        'nagiging neutral', 'nagiging maasahin', 'nagiging pesimista', \n",
    "        'nagiging nag-aalala', 'nagiging masaya', 'nagiging malungkot']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_attitudes = {\n",
    "        'Emotion-related Adjectives': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in attitude_keywords['Emotion-related Adjectives']:\n",
    "            detected_attitudes['Emotion-related Adjectives'] = True\n",
    "        \n",
    "        if token.dep_ in ['amod', 'acomp'] and token.text.lower() in attitude_keywords['Emotion-related Adjectives']:\n",
    "            detected_attitudes['Emotion-related Adjectives'] = True\n",
    "\n",
    "    return 1 if any(detected_attitudes.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Comparative Feature\n",
    "\n",
    "**Comparative** - Comparison between any aspects of two narrative events.\n",
    "\n",
    "The `extract_comparative` function identifies whether a sentence contains elements that suggest comparisons. It checks for comparative and superlative words, phrases, and related constructions. The function returns `1` if any of these indicators are present, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comparative(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        comparative_keywords = json.load(f)['comparative']\n",
=======
    "    comparative_keywords = {\n",
    "        'Comparative Adjectives': ['mas', 'higit','higit na' 'kaysa','mas mabuti', \n",
    "        'mas masama', 'mas mataas', 'mas mababa', 'mas mabilis', 'mas mabagal', 'mas matanda', \n",
    "        'mas bata', 'mas malaki', 'mas maliit', 'mas malakas', 'mas mahina', 'mas magaan', 'mas mabigat', 'mas maganda', \n",
    "        'mas pangit', 'mas malakas', 'mas mahina', 'mas mataas', 'mas mababa', 'mas bago', 'mas luma', 'mas makabago', \n",
    "        'mas konserbatibo']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_comparative = {\n",
    "        'Comparative Adjectives': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in comparative_keywords['Comparative Adjectives']:\n",
    "            detected_comparative['Comparative Adjectives'] = True\n",
    "        \n",
    "        if token.dep_ in ['amod', 'acomp'] and token.text.lower() in comparative_keywords['Comparative Adjectives']:\n",
    "            detected_comparative['Comparative Adjectives'] = True\n",
    "\n",
    "    return 1 if any(detected_comparative.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Quantifier Feature\n",
    "\n",
    "**Quantifier** - Expression of the quantity of any aspect of an event included in a narrative.\n",
    "\n",
    "The `extract_quantifier` function detects whether a sentence contains indicators of quantity or proportion. It returns `1` if quantifiers or related expressions are found, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quantifier(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        quantifier_keywords = json.load(f)['quantifier']\n",
=======
    "    quantifier_keywords = {\n",
    "        'Quantifiers': ['ang lahat', 'ilan', 'wala', 'marami', 'konti',\n",
    "        'karamihan', 'kaunti', 'kalahatan', \n",
    "        'iba', 'madami', 'mas marami', 'pinaka marami',\n",
    "        'lahat', 'marami sa', 'konti sa', 'ang lahat', 'ilang','mas']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_quantifiers = {\n",
    "        'Quantifiers': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in quantifier_keywords['Quantifiers']:\n",
    "            detected_quantifiers['Quantifiers'] = True\n",
    "        \n",
    "        if token.dep_ in ['amod', 'nummod'] and token.text.lower() in quantifier_keywords['Quantifiers']:\n",
    "            detected_quantifiers['Quantifiers'] = True\n",
    "\n",
    "    return 1 if any(detected_quantifiers.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Qualification Feature\n",
    "\n",
    "**Qualification** - Emphasis added to the description of any aspect of a narrative event.\n",
    "\n",
    "The `extract_qualification` function detects whether a sentence contains qualifiers that provide additional details or enhance the description of nouns. It returns `1` if qualifying elements are found, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qualification(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        qualification_keywords = json.load(f)['qualification']\n",
=======
    "    qualification_keywords = {\n",
    "        'Qualifying Adjectives/Adverbs': ['napaka', 'sobra', 'talaga','mas mahusay', \n",
    "        'hindi mahusay', 'magaling', 'hindi magaling', 'kasanayan', 'hindi kasanayan', 'sanay', 'hindi sanay', \n",
    "        'mahusay', 'hindi mahusay', 'dalubhasa', 'baguhan', 'bago', 'karanasan', 'walang karanasan', \n",
    "        'mas magaling', 'hindi magaling','kwalipikado', 'hindi kwalipikado']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_qualifications = {\n",
    "        'Qualifying Adjectives/Adverbs': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in qualification_keywords['Qualifying Adjectives/Adverbs']:\n",
    "            detected_qualifications['Qualifying Adjectives/Adverbs'] = True\n",
    "        \n",
    "        if token.dep_ in ['amod', 'advmod'] and token.text.lower() in qualification_keywords['Qualifying Adjectives/Adverbs']:\n",
    "            detected_qualifications['Qualifying Adjectives/Adverbs'] = True\n",
    "\n",
    "    return 1 if any(detected_qualifications.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Explanation Feature\n",
    "\n",
    "**Explanation** - Insertion of unknown information in the narrative.\n",
    "\n",
    "The `extract_explanation` function identifies whether a sentence contains explanations or clarifications. It returns `1` if explanatory elements are detected, otherwise `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_explanation(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
<<<<<<< Updated upstream
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        explanation_keywords = json.load(f)['explanation']\n",
=======
    "    explanation_keywords = {\n",
    "        'Explanation Phrases': ['dahil', 'upang', 'sapagkat','nagpaliwanag','nagpapaliwanag', 'nangatwiran', \n",
    "        'nagdiin', 'nagpakita', 'nagpapahayag', 'nagbibigay']\n",
    "    }\n",
>>>>>>> Stashed changes
    "    \n",
    "    detected_explanations = {\n",
    "        'Explanation Phrases': False\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text.lower() in explanation_keywords['Explanation Phrases']:\n",
    "            detected_explanations['Explanation Phrases'] = True\n",
    "        \n",
    "        if token.dep_ in ['mark', 'ccomp'] and token.text.lower() in explanation_keywords['Explanation Phrases']:\n",
    "            detected_explanations['Explanation Phrases'] = True\n",
    "\n",
    "    return 1 if any(detected_explanations.values()) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Feature Vector\n",
    "\n",
    "The `create_feature_vector` function generates a feature vector for a given text by extracting various narrative features. It returns a list containing binary values (1 or 0) indicating the presence of each feature."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vector(text):\n",
    "\n",
    "    mode_feature = extract_mode(text)\n",
    "    intention_feature = extract_intention(text)\n",
    "    result_feature = extract_result(text)\n",
    "    manner_feature = extract_manner(text)\n",
    "    aspect_feature = extract_aspect(text)\n",
    "    status_feature = extract_status(text)\n",
    "    appearance_feature = extract_appearance(text)\n",
    "    knowledge_feature = extract_knowledge(text)\n",
    "    description_feature = extract_description(text)\n",
    "    supposition_feature = extract_supposition(text)\n",
    "    subjectivation_feature = extract_subjectivation(text)\n",
    "    attitude_feature = extract_attitude(text)\n",
    "    comparative_feature = extract_comparative(text)\n",
    "    quantifier_feature = extract_quantifier(text)\n",
    "    qualification_feature = extract_qualification(text)\n",
    "    explanation_feature = extract_explanation(text)\n",
    "    return [\n",
    "        mode_feature, intention_feature, result_feature, manner_feature,\n",
    "        aspect_feature, status_feature, appearance_feature, knowledge_feature,\n",
    "        description_feature, supposition_feature, subjectivation_feature, attitude_feature,\n",
    "        comparative_feature, quantifier_feature, qualification_feature, explanation_feature\n",
    "        ]"
=======
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vector(sentence):\n",
    "    # Initialize NLP processing and other required resources\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Load the keywords for different features from your JSON or other sources\n",
    "    with open('../data/filipino_keywords.json', 'r', encoding='utf-8') as f:\n",
    "        keywords = json.load(f)\n",
    "    \n",
    "    # Initialize feature vector dictionary\n",
    "    feature_vector = {\n",
    "        'mode': extract_mode(sentence),\n",
    "        'intention': extract_intention(sentence),\n",
    "        'result': extract_result(sentence),\n",
    "        'manner': extract_manner(sentence),\n",
    "        'aspect': extract_aspect(sentence),\n",
    "        'status': extract_status(sentence),\n",
    "        'appearance': extract_appearance(sentence),\n",
    "        'knowledge': extract_knowledge(sentence),\n",
    "        'description': extract_description(sentence),\n",
    "        'supposition': extract_supposition(sentence),\n",
    "        'subjectivation': extract_subjectivation(sentence),\n",
    "        'attitude': extract_attitude(sentence),\n",
    "        'comparative': extract_comparative(sentence),\n",
    "        'quantifier': extract_quantifier(sentence),\n",
    "        'qualification': extract_qualification(sentence),\n",
    "        'explanation': extract_explanation(sentence)\n",
    "    }\n",
    "\n",
    "    return list(feature_vector.values())\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and CSV Export\n",
    "\n",
    "This script reads a dataset of Filipino sentences, applies feature extraction to generate feature vectors, and saves the resulting feature vectors to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<<< Updated upstream:backend/preprocessing/jupyter folder/narrative_features_fil.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [],
========
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mode  intention  result  manner  aspect  status  appearance  knowledge  \\\n",
      "0     1          1       0       0       0       0           0          0   \n",
      "1     1          0       0       0       0       1           0          0   \n",
      "2     1          0       0       0       0       0           0          0   \n",
      "3     1          0       0       0       0       0           0          0   \n",
      "4     0          1       0       0       0       0           0          0   \n",
      "5     1          1       0       0       0       0           0          0   \n",
      "6     0          1       0       0       0       0           0          0   \n",
      "7     0          1       0       1       1       0           0          0   \n",
      "8     0          1       0       0       0       0           0          0   \n",
      "\n",
      "   description  supposition  subjectivation  attitude  comparative  \\\n",
      "0            0            0               0         0            0   \n",
      "1            0            0               0         0            0   \n",
      "2            0            0               0         0            0   \n",
      "3            0            0               0         0            0   \n",
      "4            0            0               0         0            0   \n",
      "5            0            0               0         0            0   \n",
      "6            0            0               0         0            0   \n",
      "7            0            0               0         0            0   \n",
      "8            0            0               0         0            0   \n",
      "\n",
      "   quantifier  qualification  explanation  \n",
      "0           0              0            0  \n",
      "1           0              0            1  \n",
      "2           0              0            0  \n",
      "3           0              0            0  \n",
      "4           0              0            0  \n",
      "5           0              0            1  \n",
      "6           0              1            0  \n",
      "7           0              0            1  \n",
      "8           0              0            0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfeatures_df.to_csv('../data/feature_vectors_fil.csv', index=False)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>>> Stashed changes:backend/preprocessing/narrative_features_fil.ipynb
=======
<<<<<<< HEAD:backend/preprocessing/narrative_features_fil.ipynb
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'intention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/filo_feautre_sample.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_feature_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m features_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintention\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanner\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maspect\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappearance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknowledge\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupposition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubjectivation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattitude\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparative\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantifier\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqualification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplanation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(features_df)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m, in \u001b[0;36mcreate_feature_vector\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      7\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize feature vector dictionary\u001b[39;00m\n\u001b[0;32m     10\u001b[0m feature_vector \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_mode(sentence),\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintention\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mextract_intention\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_result(sentence),\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanner\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_manner(sentence),\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maspect\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_aspect(sentence),\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_status(sentence),\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappearance\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_appearance(sentence),\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknowledge\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_knowledge(sentence),\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_description(sentence),\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupposition\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_supposition(sentence),\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubjectivation\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_subjectivation(sentence),\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattitude\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_attitude(sentence),\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparative\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_comparative(sentence),\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantifier\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_quantifier(sentence),\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqualification\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_qualification(sentence),\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplanation\u001b[39m\u001b[38;5;124m'\u001b[39m: extract_explanation(sentence)\n\u001b[0;32m     27\u001b[0m }\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(feature_vector\u001b[38;5;241m.\u001b[39mvalues())\n",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m, in \u001b[0;36mextract_intention\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/filipino_keywords.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 5\u001b[0m     intention_keywords \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mintention\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m detected_intentions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInfinitive Verbs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModal Verbs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPurpose Clauses\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuxiliary Verbs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'intention'"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 3506bf63c93388de7f492d1ba532eb9a948e0f32:backend/preprocessing/jupyter folder/narrative_features_fil.ipynb
>>>>>>> Stashed changes
   "source": [
    "df = pd.read_csv('../data/filo_feautre_sample.csv')\n",
    "\n",
    "df['features'] = df['sentence'].apply(create_feature_vector)\n",
    "\n",
    "features_df = pd.DataFrame(df['features'].tolist(), columns=['mode', 'intention', 'result', 'manner',\n",
    "                                                             'aspect', 'status', 'appearance', 'knowledge',\n",
    "                                                             'description', 'supposition', 'subjectivation', 'attitude',\n",
    "                                                             'comparative', 'quantifier', 'qualification', 'explanation'])\n",
    "\n",
    "print(features_df)\n",
    "\n",
    "'''\n",
    "features_df.to_csv('../data/feature_vectors_fil.csv', index=False)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
<<<<<<<< Updated upstream:backend/preprocessing/jupyter folder/narrative_features_fil.ipynb
   "version": "3.11.9"
========
   "version": "3.12.7"
>>>>>>>> Stashed changes:backend/preprocessing/narrative_features_fil.ipynb
=======
<<<<<<< HEAD:backend/preprocessing/narrative_features_fil.ipynb
   "version": "3.12.7"
=======
   "version": "3.11.9"
>>>>>>> 3506bf63c93388de7f492d1ba532eb9a948e0f32:backend/preprocessing/jupyter folder/narrative_features_fil.ipynb
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
