{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Initial Setup\n",
    "\n",
    "The following libraries and modules are imported and initialized for text preprocessing and feature extraction:\n",
    "\n",
    "`pandas`: Library for data manipulation and analysis.\n",
    "\n",
    "`string`: Contains common string operations and constants.\n",
    "\n",
    "`nltk`: The Natural Language Toolkit, a comprehensive library for natural language processing. Here, it is used for:\n",
    "\n",
    "`nltk.tokenize.word_tokenize`: Tokenizes sentences into individual words (tokens).\n",
    "\n",
    "`nltk.corpus.stopwords`: Provides a set of common stopwords in multiple languages. These stopwords are used for filtering irrelevant words (e.g., \"the\", \"is\") from the text.\n",
    "\n",
    "`spacy`: Library for advanced natural language processing.\n",
    "\n",
    "`sklearn.feature_extraction.text.TfidfVectorizer`: For converting text to TF-IDF features.\n",
    "\n",
    "NLTK data (`punkt` and `stopwords`) is downloaded for tokenization and filtering out common stop words.\n",
    "\n",
    "The `en_core_web_trf` model from spaCy is loaded for processing text with transformers specifically for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import stopwordsiso as stopwords\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filipino Stopwords\n",
    "\n",
    "The following list contains commonly used Filipino stopwords. These are words that are typically filtered out in text processing as they do not carry significant meaning in the context of text analysis.\n",
    "\n",
    "These stopwords include common words such as articles, prepositions, and conjunctions that are usually removed during text preprocessing to focus on more meaningful words in text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filipino_stopwords = stopwords.stopwords('tl')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `load_dataset`\n",
    "\n",
    "The `load_dataset` function is used to load a dataset from a CSV file. It handles various potential errors during the file loading process.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please check the file path.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"File is empty. Please check the file content.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error parsing file. Please check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset and Extracting Sentences\n",
    "\n",
    "The following code snippet demonstrates how to load a dataset from a CSV file and extract sentences from it.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/training_data_sample.csv'\n",
    "data = load_dataset(file_path)\n",
    "\n",
    "sentences = data['sentence'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `format_list_as_string`\n",
    "\n",
    "The `format_list_as_string` function converts a list of tokens into a formatted string representation.\n",
    "\n",
    "In text preprocessing, tokens are often stored as lists of words after steps like tokenization, stopword removal, and lemmatization. These lists can sometimes be difficult to read or display, especially when reviewing or debugging processed data.\n",
    "\n",
    "this function ensures that these lists are formatted neatly when printed.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_as_string(token_list):\n",
    "    return str(token_list).replace(\"'\", '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `print_table`\n",
    "\n",
    "The `print_table` function displays a formatted table using the `rich` library, which provides enhanced terminal output for data visualization.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(data, title=\"Table\", num_samples=20):\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    \n",
    "    table = Table(title=title)\n",
    "    \n",
    "    for col in data.columns:\n",
    "        table.add_column(col)\n",
    "\n",
    "    for _, row in data.head(num_samples).iterrows():\n",
    "        formatted_row = [format_list_as_string(row[col]) if isinstance(row[col], list) else row[col] for col in data.columns]\n",
    "        table.add_row(*map(str, formatted_row))\n",
    "    \n",
    "    console = Console()\n",
    "    console.print(table)\n",
    "\n",
    "print_table(data, title=\"Original Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `convert_to_lowercase`\n",
    "\n",
    "The `convert_to_lowercase` function converts all text in the 'sentence' column of a DataFrame to lowercase.\n",
    "\n",
    "It ensures uniformity in text data by transforming all characters to lowercase, which is a key preprocessing step in natural language processing (NLP) tasks.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lowercase(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].str.lower()\n",
    "        print(\"Sentence has been converted to lowercase.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "convert_to_lowercase(data)\n",
    "print_table(data, title=\"Data After Lowercase Conversion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `remove_punctuation`\n",
    "\n",
    "The `remove_punctuation` function removes punctuation from all text in the 'sentence' column of a DataFrame.\n",
    "\n",
    "It ensures that only the meaningful words remain, free of any punctuation that could interfere with subsequent text processing steps.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "        print(\"Punctuation has been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_punctuation(data)\n",
    "print_table(data, title=\"Data After Punctuation Removal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `remove_numbers`\n",
    "\n",
    "The `remove_numbers` function removes numerical digits from all text in the 'sentence' column of a DataFrame.\n",
    "\n",
    "It ensures that the text data is free from irrelevant numerical characters that could distort the analysis.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].str.replace(r'\\d+', '', regex=True)\n",
    "        print(\"Numbers have been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_numbers(data)\n",
    "print_table(data, title=\"Data After Numbers Removal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `tokenize_sentences`\n",
    "\n",
    "The `tokenize_sentences` function tokenizes each sentence in the 'sentence' column of a DataFrame into individual words.\n",
    "\n",
    "It breaks down sentences into individual words or tokens, facilitating analysis, standardizing input, and enabling various natural language processing tasks.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda x: word_tokenize(x))\n",
    "        print(\"Sentences have been tokenized.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "tokenize_sentences(data)\n",
    "print_table(data, title=\"Data After Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `remove_stopwords`\n",
    "\n",
    "The `remove_stopwords` function removes stopwords from each tokenized sentence in the 'sentence' column of a DataFrame.\n",
    "\n",
    "Stopwords are common words that usually add little meaning to a sentence and are often filtered out in text processing to enhance the focus on more meaningful words.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        english_stopwords = set(stopwords.stopwords('english'))\n",
    "        all_stopwords = english_stopwords.union(set(filipino_stopwords))\n",
    "        \n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: [word for word in tokens if word.lower() not in all_stopwords])\n",
    "        print(\"Stopwords have been removed.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "remove_stopwords(data)\n",
    "print_table(data, title=\"Data After Stopwords Removal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `lemmatize_tokens`\n",
    "\n",
    "The `lemmatize_tokens` function lemmatizes each token in the 'sentence' column of a DataFrame using spaCy's lemmatization.\n",
    "\n",
    "it reduces words to their base or dictionary form (lemmas), which helps in standardizing the text by consolidating different inflections of a word into a single representation, thereby improving the performance of natural language processing tasks such as classification and clustering.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_filo(data):\n",
    "    with open('../data/filipino_lemmatizer.json', 'r', encoding='utf-8') as json_file:\n",
    "        lemma_dict = json.load(json_file)\n",
    "\n",
    "    token_to_lemma = {}\n",
    "    for lemma, tokenval in lemma_dict['lemma_dict'].items():\n",
    "        for token in tokenval:\n",
    "            token_to_lemma[token] = lemma\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        tokens = row['sentence']\n",
    "        \n",
    "        if all(len(token) == 1 for token in tokens):\n",
    "            tokens = ''.join(tokens).split()\n",
    "        \n",
    "        updated_tokens = [token_to_lemma.get(token, token) for token in tokens]\n",
    "        data.at[index, 'sentence'] = updated_tokens\n",
    "\n",
    "lemmatize_filo(data)\n",
    "print_table(data, title=\"Data After Lemmatization in Filipino\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_eng(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: [nlp(token)[0].lemma_ for token in tokens])\n",
    "        print(\"Tokens have been lemmatized.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "lemmatize_eng(data)\n",
    "print_table(data, title=\"Data After Lemmatization in English\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `join_tokens`\n",
    "\n",
    "The `join_tokens` function joins tokens in each list within the 'sentence' column of a DataFrame back into single sentences.\n",
    "\n",
    "it reconstructs the original text format after various transformations, enabling further analysis or modeling tasks that require complete sentences, such as text classification, sentiment analysis, or readability assessments.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        data['sentence'] = data['sentence'].apply(lambda tokens: ' '.join(tokens))\n",
    "        print(\"Tokens have been joined back into sentences.\")\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "join_tokens(data)\n",
    "print_table(data, title=\"Data After Joining Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `vectorize_with_tfidf`\n",
    "\n",
    "The `vectorize_with_tfidf` function performs TF-IDF vectorization on the 'sentence' column of a DataFrame.\n",
    "\n",
    "**TF-IDF** is a statistical measure used to evaluate the importance of a word in a document relative to a collection (or corpus) of documents. It combines two key concepts:\n",
    "\n",
    "1. **Term Frequency (TF)**: Measures how frequently a word appears in a single document. Words that appear frequently within a document are considered more important for that document.\n",
    "   - Formula: `TF = (Number of times a term appears in a document) / (Total number of terms in the document)`\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: Measures how important a word is across the entire dataset. Words that appear in many documents (common words) are less important, while rare words are more important.\n",
    "   - Formula: `IDF = log(Total number of documents / Number of documents containing the term)`\n",
    "\n",
    "The **TF-IDF score** is calculated by multiplying TF and IDF for each word, providing a numerical value representing the word's significance within a document relative to the entire corpus.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_with_tfidf(data):\n",
    "    if 'sentence' in data.columns:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "        tfidf_matrix = vectorizer.fit_transform(data['sentence'])\n",
    "\n",
    "        print(\"TF-IDF Vectorization complete.\")\n",
    "        return tfidf_matrix, vectorizer\n",
    "    else:\n",
    "        print(\"Column 'sentence' not found in the DataFrame.\")\n",
    "\n",
    "tfidf_matrix, vectorizer = vectorize_with_tfidf(data)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "tfidf_df['emotion'] = data['emotion'].values\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "tfidf_df.to_csv('tfidf_vectorized_data.csv', index=False)\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_model(model, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "models_folder = os.path.join(current_dir, \"..\", \"models\")\n",
    "if not os.path.exists(models_folder):\n",
    "    os.makedirs(models_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the TF-IDF Vectorizer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(vectorizer, os.path.join(models_folder, \"tfidf_vectorizer_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: `RandomOverSampler` and `RandomUnderSampler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df.drop(columns=['emotion'])\n",
    "y = tfidf_df['emotion']\n",
    "\n",
    "print(\"Class distribution before resampling:\")\n",
    "print(y.value_counts())\n",
    "print(\"\\nData before resampling:\")\n",
    "print(tfidf_df)\n",
    "\n",
    "def resample_data(X, y):\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_over, y_over = oversampler.fit_resample(X, y)\n",
    "    \n",
    "    print(f\"\\nOversampled class distribution:\\n{y_over.value_counts()}\")\n",
    "\n",
    "    undersampler = RandomUnderSampler(random_state=42)\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X_over, y_over)\n",
    "    \n",
    "    print(f\"\\nFinal resampled class distribution:\\n{y_resampled.value_counts()}\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "X_resampled, y_resampled = resample_data(X, y)\n",
    "\n",
    "resampled_df = pd.concat([pd.DataFrame(X_resampled, columns=feature_names), pd.DataFrame(y_resampled, columns=['emotion'])], axis=1)\n",
    "\n",
    "resampled_df.to_csv('../data/resampled_df.csv', index=False)\n",
    "\n",
    "print(\"\\nData after resampling:\")\n",
    "print(resampled_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
